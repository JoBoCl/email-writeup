\chapter{Evaluation}\label{chap:test}
After producing the application described in Chapter~\ref{chap:imp}, referring
back to the design specifications to determine its performance against the
stated criteria was the next step.  

\section{Methodology}
Using a sample of e-mails provided by my supervisor, each of them was run
through the final version of the program, and scored based on the following
attributes:

\begin{itemize}
\item Let $M$ be the total number of received fields.
\item Let $N$ be the total number of other fields.
\item 1 point is added to the total $R$ for each piece of information found in a Received field for the following:
\subitem{One of device name \emph{or} IP address}
\subitem{Software \emph{or} protocol used}
\subitem{Vulnerabilities found for the relevant piece of software}
\subitem{Location Data}
\item 1 point is addded to $F$ for each piece of information found in other fields.
\end{itemize}

The final score for an e-mail is given as \[\frac12\left(\frac R{4M}+\frac
FN\right)\]to give a value between 0 and 1 for each e-mail.

The e-mails received have been numbered from 1 up to 70, and a random sample of
size 30 was selected using the following code:

\begin{verbatim}
>>> random.seed()
>>> random.sample(list(range(1,70)), 30)
[64, 48, 11, 21, 63, 68, 27, 69, 29,  8, 28,
 34, 13, 57, 10,  3, 22, 32, 23, 49, 26, 45,
 19,  1, 36, 46, 41, 18, 20, 17]
\end{verbatim}

Thus giving a sorted list of the following e-mails: 1, 3, 8, 10, 11, 13, 17,
18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 32, 34, 36, 41, 45, 46, 48, 49, 57, 63,
64, 68, 69.

\section{Sample Output}

The following pages show the results of running the completed software on the
e-mail labelled \texttt{11.txt}, the contents of which is listed in
Fragment~\ref{eg:11}.  The full table of CVE entries is elided for brevity.  
The entries in \colorbox{red!30}{this colour} are associated with points scored
for $R$, and the entries in \colorbox{blue!30}{this colour} are associated with
points scored for $F$.

\begin{example}[caption=Email \texttt{11.txt},label=eg:11]
Received: from relay13.mail.ox.ac.uk (129.67.1.163) by HUB01.ad.oak.ox.ac.uk 
 (|\colorbox{red!30}{163.1.154.218}|) with |\colorbox{red!30}{Microsoft SMTP Server}| id 14.3.169.1; Wed, 1 Apr 2015 
 11:06:18 +0100 
Received: from postie2.cs.ox.ac.uk ([129.67.151.44])	by |\colorbox{red!30}{relay12.mail.ox.ac.uk }|
 with |\colorbox{red!30}{esmtp (Exim 4.80}|)	(envelope-from <ahayes@mays.tamu.edu>)	id 
 1YdFXO-0008P8-de	for cccc1111@nexus.ox.ac.uk; Wed, 01 Apr 2015 11:06:18 +0100 
Received: from mailer.cs.ox.ac.uk ([129.67.151.81]:36787)	by 
 |\colorbox{red!30}{postie2.cs.ox.ac.uk}| with |\colorbox{red!30}{esmtp (Exim 4.72)}|	(envelope-from 
 <ahayes@mays.tamu.edu>)	id 1YdFWT-0004Fk-DZ	for jason.nurse@cs.ox.ac.uk; Wed, 
 01 Apr 2015 11:05:21 +0100 
Received: from relay11.mail.ox.ac.uk ([129.67.1.162]:57950)	by 
|\colorbox{red!30}{mailer.cs.ox.ac.uk}| with |\colorbox{red!30}{esmtp (Exim 4.72)}|	(envelope-from 
 <ahayes@mays.tamu.edu>)	id 1YdFWS-0003OY-6C	for jason.nurse@cs.ox.ac.uk; Wed, 
 01 Apr 2015 11:05:20 +0100 
Received: from mailbox2.mbs.tamu.edu ([128.194.216.125])	by 
|\colorbox{red!30}{relay11.mail.ox.ac.uk}| with |\colorbox{red!30}{esmtp (Exim 4.80)}|	(envelope-from 
 <ahayes@mays.tamu.edu>)	id 1YdFWS-0000Ck-Zf	for jason.nurse@cs.ox.ac.uk; Wed, 
 01 Apr 2015 11:05:20 +0100 
Received: from MAILBOX1.mbs.tamu.edu ([|\colorbox{red!30}{169.254.2.132}|]) by
MAILBOX2.mbs.tamu.edu ([|\colorbox{red!30}{169.254.1.80}|]) with |\colorbox{red!30}{mapi}| id 14.03.0224.002; Wed, 1
Apr 2015 05:05:00 -0500
From: |\colorbox{blue!30}{"Hayes, Allison" <ahayes@mays.tamu.edu>}|
To: "Hayes, Allison" <ahayes@mays.tamu.edu>
Subject: RE: ITS HELP DESK
Thread-Topic: ITS HELP DESK
Thread-Index: AdBsXNo2iPt2JIAvQzCkSflypfIvlgABOhlv
Date: Wed, 1 Apr 2015 10:04:57 +0000 
Message-ID: <3AC6BF6FEAFA734A8A0397E31CB7AD009B695F@MAILBOX1.mbs.tamu.edu> 
References: <3AC6BF6FEAFA734A8A0397E31CB7AD009A5A35@MAILBOX1.mbs.tamu.edu> 
In-Reply-To: <3AC6BF6FEAFA734A8A0397E31CB7AD009A5A35@MAILBOX1.mbs.tamu.edu> 
|\colorbox{blue!30}{Accept-Language: en-US }|
Content-Language: en-US 
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [208.76.111.246] 
Content-Type: multipart/alternative; 
	boundary="_000_3AC6BF6FEAFA734A8A0397E31CB7AD009B695FMAILBOX1mbstamued_" 
MIME-Version: 1.0 
X-Oxmail-Spam-Status: score=2.9 tests=HTML_MESSAGE,SUBJ_ALL_CAPS,T_RP_MATCHES_RCVD,URI_HEX 
X-Oxmail-Spam-Level: ** 
Return-Path: ahayes@mays.tamu.edu 
|\colorbox{blue!30}{X-MS-Exchange-Organization-AuthSource: HUB01.ad.oak.ox.ac.uk }|
|\colorbox{blue!30}{X-MS-Exchange-Organization-AuthAs: Anonymous }|
|\colorbox{blue!30}{X-MS-Exchange-Organization-AVStamp-Mailbox: Sophos;-2052447998;0;PM }|
|\colorbox{blue!30}{X-MS-Exchange-Organization-SCL: 2 }|
\end{example}

\includepdf[pages={1,2,3}]{11_results.pdf}

\section{Results}

Table~\ref{tab:sammn} gives the $M$ and $N$ values for the different sampled
e-mails.  These were sampled using standard Unix tools: \texttt{grep}ping for
fields and counting the output.

\begin{table}[]
\centering
\begin{tabular}{@{}lccclll@{}}
\toprule
Header & Total Fields &                  $M$  & $N$  & $R$ & $F$ & $S$ \\ \midrule
1.txt  & 26           & 7                     & 19   & 23  & 7   &     \\
3.txt  & 32           & 6                     & 26   & 16  & 6   &     \\
8.txt  & 34           & 8                     & 26   & 30  & 6   &     \\
10.txt & 22           & 3                     & 19   & 9   & 7   &     \\
11.txt & 29           & 6                     & 23   & 23  & 6   &     \\
13.txt & 17           & 4                     & 13   & 13  & 5   &     \\
17.txt & 27           & 6                     & 21   & 22  & 6   &     \\
18.txt & 20           & 6                     & 14   & 23  & 5   &     \\
19.txt & 20           & 6                     & 14   & 21  & 4   &     \\
20.txt & 22           & 7                     & 15   & 25  & 7   &     \\
21.txt & 22           & 6                     & 16   & 23  & 5   &     \\
22.txt & 22           & 6                     & 16   & 22  & 6   &     \\
23.txt & 26           & 6                     & 20   & 20  & 7   &     \\
26.txt & 19           & 6                     & 13   & 21  & 5   &     \\
27.txt & 21           & 1                     & 20   & 2   & 8   &     \\
28.txt & 22           & 6                     & 16   &     &     &     \\
29.txt & 20           & 6                     & 14   &     &     &     \\
32.txt & 24           & 6                     & 18   &     &     &     \\
34.txt & 23           & 5                     & 18   &     &     &     \\
36.txt & 26           & 6                     & 20   &     &     &     \\
41.txt & 21           & 6                     & 15   &     &     &     \\
45.txt & 23           & 6                     & 17   &     &     &     \\
46.txt & 26           & 5                     & 21   &     &     &     \\
48.txt & 25           & 6                     & 19   &     &     &     \\
49.txt & 21           & 6                     & 15   &     &     &     \\
57.txt & 28           & 6                     & 22   &     &     &     \\
63.txt & 23           & 5                     & 18   &     &     &     \\
64.txt & 36           & 9                     & 27   & 31  & 6   &     \\
68.txt & 21           & 6                     & 15   &     &     &     \\
69.txt & 22           & 6                     & 16   &     &     &     \\ \bottomrule
\end{tabular}
\caption{$M$, $N$, $R$, $F$ and score values for chosen headers}
\label{tab:sammn}
\end{table}

The results of the testing are contained within Table~\ref{tab:res}, and give a
breakdown for each e-mail's values of $R$ and $F$, as well as the final score.

During the testing, the following trends were noticed.  As many of the e-mails
passed through the same set of servers, as they had been received by an Oxford
e-mail address, the same set of servers were frequently seen, all of which had
associated IP addresses, geolocation data and (except for one server running
Microsoft SMTP Server) CVE data for the running software.  For an e-mail sent
within the University Nexus system, this gives an inflated score, as very few
of the servers are missing information.

The score for the information gathered from fields does not take into account
the number of fields needed to determine or infer a piece of information. For
example, the presence, or absence of multiple fields is required to determine a
piece of information.  Nor does it consider the relative value of a piece of
information, failing to rate the presence of a username above the presence of a
particular piece of software also giving false positives.

However, very few e-mails in the testing population contained fields relating
to usernames (for example, \texttt{X-}$\ldots$\texttt{-User}, \texttt{X-Oxford-Username},
\texttt{X-Authenticated-User}) compared to the result
in~\cite{nurse2015investigating}, which found 14\% of e-mails to contain
usernames as opposed to the 8\% found in the population.

\cleardoublepage \chapter{Evaluation}
\section{Conclusions}

As described in Chapter~\ref{chap:int}, the aim of this project is to support a
better understanding of the data that may leaked when e-mails are sent, both
from a personal perspective, as well as the corporate data that is leaked
concern network configurations and software installations.

To support this, I have developed a tool that can be used to automatically
extract information from e-mail headers and analyse its results to display the
personal information contained within an e-mail's header, as well as
information about the software configurations that may be found on a user's
computer, or the servers used to send their e-mail.


\section{Future Work}

During the late stages of development and testing, a number of missing features
quickly became apparent. Due to the limited information available, and the
differences in version numbering, a decision was made to search for all
available vulnerabilities for an application, allowing the user to discern
which were most relevant.  Subsequent versions could focus on the different
pieces of version data available.  For example, \texttt{esmtp} frequently
references its version number in the ``Received'' field frequently.

Alternatively, a better picture may be presented by accumulating multiple
e-mails. For example, using the information provided from multiple members of
single organisation, a better picture may be built up of the software used by
the servers, as well as the network configuration.

The application's response times may also be improved by caching some data in
memory, such as WhoIs responses and GeoIP lookups, so that frequently accessed
lookups can be completed more quickly.

Additionally, future testing should take place on a larger dataset, using
e-mails from a wider variety of sources sent to a number of different
recipients.  

Finally, it should be possible for an updated version of this application to
determine which header fields have been added by mail servers within one's own
organisation.  For example, e-mail header fields beginning with
\texttt{X-MS-Exchange} are seen within almost all e-mail messages sent to
recipients within the Oxford domain, adding more false positives to the test
results. While it is possible that other preceding e-mail servers have added
similar fields, in most cases, these entries yielded little useful data.

